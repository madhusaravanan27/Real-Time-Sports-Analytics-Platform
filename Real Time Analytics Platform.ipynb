{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3254652-59ff-490d-9e2d-00d9d77e2baf",
      "metadata": {
        "id": "f3254652-59ff-490d-9e2d-00d9d77e2baf",
        "outputId": "96ce4bf0-bc37-44d1-d7a5-a763f00d17dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
            "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
            "org.apache.hadoop#hadoop-aws added as a dependency\n",
            "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
            "com.microsoft.azure#spark-mssql-connector_2.12 added as a dependency\n",
            "com.microsoft.sqlserver#mssql-jdbc added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-af34e8b2-f191-461e-9c8f-bdea2192d86a;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.hadoop#hadoop-aws;3.1.2 in central\n",
            "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.271 in central\n",
            "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
            "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
            "\tfound org.mongodb#bson;4.0.5 in central\n",
            "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
            "\tfound com.microsoft.azure#spark-mssql-connector_2.12;1.2.0 in central\n",
            "\tfound com.microsoft.sqlserver#mssql-jdbc;12.2.0.jre11 in central\n",
            ":: resolution report :: resolve 932ms :: artifacts dl 20ms\n",
            "\t:: modules in use:\n",
            "\tcom.amazonaws#aws-java-sdk-bundle;1.11.271 from central in [default]\n",
            "\tcom.microsoft.azure#spark-mssql-connector_2.12;1.2.0 from central in [default]\n",
            "\tcom.microsoft.sqlserver#mssql-jdbc;12.2.0.jre11 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-aws;3.1.2 from central in [default]\n",
            "\torg.mongodb#bson;4.0.5 from central in [default]\n",
            "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
            "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
            "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-af34e8b2-f191-461e-9c8f-bdea2192d86a\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 8 already retrieved (0kB/16ms)\n",
            "24/03/08 21:42:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        }
      ],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "user = \"mongo\"\n",
        "passwd = \"SU2orange!\"\n",
        "s3_bucket = \"gamestreams\"\n",
        "s3_server = \"http://minio:9000\"\n",
        "s3_access_key = \"minio\"\n",
        "s3_secret_key = \"SU2orange!\"\n",
        "mongo_uri = f\"mongodb://{user}:{passwd}@mongo:27017/admin?authSource=admin\"\n",
        "server_name = \"jdbc:sqlserver://mssql\"\n",
        "database_name = \"sidearmdb\"\n",
        "mssql_user = \"sa\"\n",
        "mssql_pw = \"SU2orange!\"\n",
        "mssql_url = \"jdbc:sqlserver://mssql:1433;databaseName=sidearmdb\" + \";\" + \"databaseName=\" + \"sidearmdb\" + \";encrypt=true;trustServerCertificate=true;\"\n",
        "\n",
        "jars = [\n",
        "    \"org.apache.hadoop:hadoop-aws:3.1.2\",\n",
        "    \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\",\n",
        "    \"com.microsoft.azure:spark-mssql-connector_2.12:1.2.0\",\n",
        "    \"com.microsoft.sqlserver:mssql-jdbc:12.2.0.jre11\"\n",
        "]\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName('jupyter-pyspark') \\\n",
        "        .config(\"spark.jars.packages\",\",\".join(jars) )\\\n",
        "        .config(\"spark.hadoop.fs.s3a.endpoint\", s3_server ) \\\n",
        "        .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key) \\\n",
        "        .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_key) \\\n",
        "        .config(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
        "        .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
        "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "        .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
        "        .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"ERROR\") # Keeps the noise down!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d93a740-0b7c-410f-a8ed-2ad5734c34c8",
      "metadata": {
        "id": "0d93a740-0b7c-410f-a8ed-2ad5734c34c8",
        "outputId": "ccf1bc71-66d7-4fe1-c14a-743a41a237d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 0:===========================================================(1 + 0) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+-----+-----+------+\n",
            "| id|  name|number|shots|goals|teamid|\n",
            "+---+------+------+-----+-----+------+\n",
            "|  1|   sam|     6|   56|   23|   101|\n",
            "|  2| sarah|     1|   85|   34|   101|\n",
            "|  3| steve|     2|   60|   20|   101|\n",
            "|  4| stone|    13|   33|   10|   101|\n",
            "|  5|  sean|    17|   26|    9|   101|\n",
            "|  6|   sly|     8|   78|   15|   101|\n",
            "|  7|   sol|     9|   52|   20|   101|\n",
            "|  8| shree|     4|   20|    4|   101|\n",
            "|  9|shelly|    15|   10|    2|   101|\n",
            "| 10| swede|    10|   90|   50|   101|\n",
            "| 11| jimmy|     1|  100|   50|   205|\n",
            "| 12| julie|     9|   10|    0|   205|\n",
            "| 13| james|     2|   45|   15|   205|\n",
            "| 14|  jane|    15|   82|   46|   205|\n",
            "| 15| jimmy|    16|   42|   30|   205|\n",
            "| 16| julie|     8|   67|   32|   205|\n",
            "| 17| james|    17|   40|   14|   205|\n",
            "| 18|  jane|     3|   91|   40|   205|\n",
            "| 19| jimmy|     5|   78|   22|   205|\n",
            "| 20| julie|    22|   83|   19|   205|\n",
            "+---+------+------+-----+-----+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# HOW TO READ FROM MSSQL\n",
        "df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"players\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .load()\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2486f5fd-62fe-41c7-b042-53098cb6da0b",
      "metadata": {
        "id": "2486f5fd-62fe-41c7-b042-53098cb6da0b",
        "outputId": "cad6c7c3-fac0-4629-eddf-a9d7d2f6d506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+----------+----+------+\n",
            "| id|name|conference|wins|losses|\n",
            "+---+----+----------+----+------+\n",
            "+---+----+----------+----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# HOW TO WRITE TO MSSQL\n",
        "df.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"players2\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .save()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60bb340d-d6db-419e-9eb2-55f53f079cad",
      "metadata": {
        "id": "60bb340d-d6db-419e-9eb2-55f53f079cad",
        "outputId": "e84f201c-1e5f-4cae-a1e7-94107b417d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------------+------+------------+----+\n",
            "|eventId|eventTimestamp|teamId|jerseyNumber|goal|\n",
            "+-------+--------------+------+------------+----+\n",
            "|      0|         59:51|   101|           2|   0|\n",
            "|      1|         57:06|   101|           6|   0|\n",
            "|      2|         56:13|   205|           8|   1|\n",
            "|      3|         55:25|   101|           4|   0|\n",
            "|      4|         55:03|   101|           1|   1|\n",
            "|      5|         54:50|   101|          17|   0|\n",
            "|      6|         54:14|   205|           8|   0|\n",
            "|      7|         53:59|   101|           9|   0|\n",
            "|      8|         53:23|   101|           2|   0|\n",
            "|      9|         51:21|   101|          13|   0|\n",
            "|     10|         49:55|   101|           1|   1|\n",
            "|     11|         49:28|   101|           2|   1|\n",
            "|     12|         48:52|   101|          10|   1|\n",
            "|     13|         47:52|   101|           4|   1|\n",
            "|     14|         47:44|   101|           9|   0|\n",
            "|     15|         46:38|   101|           2|   0|\n",
            "|     16|         45:49|   101|           1|   1|\n",
            "|     17|         45:31|   101|           4|   0|\n",
            "|     18|         43:29|   205|           1|   1|\n",
            "|     19|         41:54|   205|           1|   1|\n",
            "+-------+--------------+------+------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: Read the gamestream.txt from minio\n",
        "from pyspark.sql.functions import col\n",
        "gamestream = spark.read.text(f\"s3a://gamestreams/gamestream.txt\")\n",
        "#gamestream.show()\n",
        "#gamestream.printSchema()\n",
        "\n",
        "\n",
        "dfNew= gamestream.selectExpr(\n",
        "  \"split(value, '[ ]')[0] as eventId\",\n",
        "  \"split(value, '[ ]')[1] as eventTimestamp\",\n",
        "  \"split(value, '[ ]')[2] as teamId\",\n",
        "  \"split(value, '[ ]')[3] as jerseyNumber\",\n",
        "  \"split(value, '[ ]')[4] as goal\")\n",
        "#dfNew.show()\n",
        "\n",
        "dfNew = dfNew.withColumn(\"eventId\", col(\"eventId\").cast(\"int\"))\\\n",
        "                     .withColumn(\"eventTimestamp\", col(\"eventTimestamp\").cast(\"string\"))\\\n",
        "                     .withColumn(\"teamId\", col(\"teamId\").cast(\"int\"))\\\n",
        "                     .withColumn(\"jerseyNumber\", col(\"jerseyNumber\").cast(\"int\"))\\\n",
        "                     .withColumn(\"goal\", col(\"goal\").cast(\"int\"))\n",
        "\n",
        "dfNew.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154a00a5-f0dd-4548-be42-60be34871f47",
      "metadata": {
        "id": "154a00a5-f0dd-4548-be42-60be34871f47",
        "outputId": "8efa563e-f888-4d34-c355-33163f35cbab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 13:===============================================>        (64 + 1) / 75]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------------+----------+----------+--------------+\n",
            "|teamId|jerseyNumber|totalGoals|totalShots|totalTeamGoals|\n",
            "+------+------------+----------+----------+--------------+\n",
            "|   205|           9|         0|         4|             9|\n",
            "|   205|           2|         1|         3|             9|\n",
            "|   101|           9|         0|         5|            14|\n",
            "|     0|           0|         0|         1|             0|\n",
            "|   101|          10|         1|         3|            14|\n",
            "|   101|           2|         2|         7|            14|\n",
            "|   101|           8|         0|         4|            14|\n",
            "|   205|          16|         0|         1|             9|\n",
            "|   101|          13|         1|         7|            14|\n",
            "|   101|           6|         2|         4|            14|\n",
            "|   101|          15|         1|         3|            14|\n",
            "|   205|           3|         0|         1|             9|\n",
            "|   205|          15|         2|         2|             9|\n",
            "|   205|           5|         1|         2|             9|\n",
            "|   101|           1|         6|         8|            14|\n",
            "|   205|           1|         3|         3|             9|\n",
            "|   205|          22|         0|         1|             9|\n",
            "|   205|          17|         1|         3|             9|\n",
            "|   205|           8|         1|         2|             9|\n",
            "|   101|           4|         1|         5|            14|\n",
            "+------+------------+----------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from pyspark.sql.functions import col, sum as spark_sum,count\n",
        "\n",
        "playerDF = dfNew.groupBy(\"teamId\", \"jerseyNumber\") \\\n",
        "    .agg(spark_sum(\"goal\").alias(\"totalGoals\"), count(col(\"goal\").cast(\"int\")).alias(\"totalShots\"))\n",
        "\n",
        "\n",
        "teamDF = dfNew.groupBy(\"teamId\") \\\n",
        "    .agg(spark_sum(\"goal\").alias(\"totalTeamGoals\"))\n",
        "\n",
        "finalstatsdf = playerDF.join(teamDF, \"teamId\", \"inner\")\n",
        "\n",
        "finalstatsdf.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947c9ee9-4047-4408-9417-515877a8879e",
      "metadata": {
        "id": "947c9ee9-4047-4408-9417-515877a8879e",
        "outputId": "b7092585-45eb-4759-c5d8-83110e877848"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------------+----------+----------+--------------+---------------+----------------------+\n",
            "|teamId|jerseyNumber|totalGoals|totalShots|totalTeamGoals|latest_event_id|latest_event_timestamp|\n",
            "+------+------------+----------+----------+--------------+---------------+----------------------+\n",
            "|   101|           9|         0|         5|            14|             69|                 59:51|\n",
            "|   101|           9|         0|         5|            14|             69|                 59:51|\n",
            "|   101|           9|         0|         5|            14|             69|                 59:51|\n",
            "|   101|           9|         0|         5|            14|             69|                 59:51|\n",
            "|   101|           9|         0|         5|            14|             69|                 59:51|\n",
            "|   101|          10|         1|         3|            14|             69|                 59:51|\n",
            "|   101|          10|         1|         3|            14|             69|                 59:51|\n",
            "|   101|          10|         1|         3|            14|             69|                 59:51|\n",
            "|   101|           2|         2|         7|            14|             69|                 59:51|\n",
            "|   101|           2|         2|         7|            14|             69|                 59:51|\n",
            "|   101|           2|         2|         7|            14|             69|                 59:51|\n",
            "|   101|           2|         2|         7|            14|             69|                 59:51|\n",
            "|   101|           2|         2|         7|            14|             69|                 59:51|\n",
            "|   101|           2|         2|         7|            14|             69|                 59:51|\n",
            "|   101|           2|         2|         7|            14|             69|                 59:51|\n",
            "|   101|           8|         0|         4|            14|             69|                 59:51|\n",
            "|   101|           8|         0|         4|            14|             69|                 59:51|\n",
            "|   101|           8|         0|         4|            14|             69|                 59:51|\n",
            "|   101|           8|         0|         4|            14|             69|                 59:51|\n",
            "|   101|          13|         1|         7|            14|             69|                 59:51|\n",
            "+------+------------+----------+----------+--------------+---------------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Joining the two DataFrames\n",
        "joineddf = dfNew.join(finalstatsdf, [\"teamId\", \"jerseyNumber\"], \"inner\")\n",
        "\n",
        "\n",
        "latest_event_df = joineddf.groupBy(\"teamId\").agg(\n",
        "    F.max(\"eventId\").alias(\"latest_event_id\"),\n",
        "    F.max(\"eventTimestamp\").alias(\"latest_event_timestamp\")\n",
        ")\n",
        "\n",
        "latest_event_df = latest_event_df.dropDuplicates([\"teamId\"])\n",
        "\n",
        "resultdf = joineddf.join(\n",
        "    latest_event_df,\n",
        "    joineddf.teamId == latest_event_df.teamId,\n",
        "    \"inner\"\n",
        ").select(\n",
        "    joineddf[\"teamId\"],\n",
        "    joineddf[\"jerseyNumber\"],\n",
        "    joineddf[\"eventId\"],\n",
        "    joineddf[\"eventTimestamp\"],\n",
        "    joineddf[\"totalGoals\"],\n",
        "    joineddf[\"totalShots\"],\n",
        "    joineddf[\"totalTeamGoals\"],\n",
        "    latest_event_df[\"latest_event_id\"],\n",
        "    latest_event_df[\"latest_event_timestamp\"]\n",
        ")\n",
        "\n",
        "\n",
        "resultdf.select(\n",
        "    joineddf[\"teamId\"],\n",
        "    joineddf[\"jerseyNumber\"],\n",
        "    joineddf[\"totalGoals\"],\n",
        "    joineddf[\"totalShots\"],\n",
        "    joineddf[\"totalTeamGoals\"],\n",
        "    latest_event_df[\"latest_event_id\"],\n",
        "    latest_event_df[\"latest_event_timestamp\"]\n",
        ").show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f02bccb-b6cd-48db-b366-2d0d15e85e66",
      "metadata": {
        "id": "0f02bccb-b6cd-48db-b366-2d0d15e85e66",
        "outputId": "61a51f49-3264-4535-c765-dd7a7cc02201"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------------+-------+--------------+----------+----------+--------------+---------------+----------------------+--------+-----+--------+----------+----+------+\n",
            "|teamId|jerseyNumber|eventId|eventTimestamp|totalGoals|totalShots|totalTeamGoals|latest_event_id|latest_event_timestamp|playerId| name|teamName|conference|wins|losses|\n",
            "+------+------------+-------+--------------+----------+----------+--------------+---------------+----------------------+--------+-----+--------+----------+----+------+\n",
            "|   101|           1|     67|         03:22|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     67|         03:22|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     54|         18:24|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     54|         18:24|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     52|         20:22|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     52|         20:22|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     24|         40:20|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     24|         40:20|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     20|         41:09|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     20|         41:09|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     16|         45:49|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     16|         45:49|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     10|         49:55|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|   101|           1|     10|         49:55|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|   101|           1|      4|         55:03|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|   101|           1|      4|         55:03|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|   101|          13|     68|         00:42|         1|         7|            14|             69|                 59:51|       4|stone|syracuse|       acc|  11|     2|\n",
            "|   101|          13|     63|         07:42|         1|         7|            14|             69|                 59:51|       4|stone|syracuse|       acc|  11|     2|\n",
            "|   101|          13|     46|         25:17|         1|         7|            14|             69|                 59:51|       4|stone|syracuse|       acc|  11|     2|\n",
            "|   101|          13|     39|         30:26|         1|         7|            14|             69|                 59:51|       4|stone|syracuse|       acc|  11|     2|\n",
            "+------+------------+-------+--------------+----------+----------+--------------+---------------+----------------------+--------+-----+--------+----------+----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "teamsdf = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"teams\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .load()\n",
        "\n",
        "boxPlayers = resultdf.join(df,\n",
        "                           (resultdf.jerseyNumber == df.number),  # Add comma here\n",
        "                           \"left\") \\\n",
        "                     .select(resultdf[\"*\"], df[\"id\"].alias(\"playerId\"), df[\"name\"])\n",
        "\n",
        "# Joining team statistics with team reference data\n",
        "boxScores = boxPlayers.join(teamsdf, boxPlayers.teamId == teamsdf.id, \"inner\") \\\n",
        "    .select(boxPlayers[\"*\"], teamsdf[\"name\"].alias(\"teamName\"), teamsdf[\"conference\"], teamsdf[\"wins\"], teamsdf[\"losses\"])\n",
        "\n",
        "\n",
        "boxScores.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8c4ddc-fdc9-4c0f-ad7e-28fa883eb9de",
      "metadata": {
        "id": "4a8c4ddc-fdc9-4c0f-ad7e-28fa883eb9de",
        "outputId": "17d4562f-fdb3-46b9-8f51-c742afb705cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 114:=================================================>   (187 + 1) / 200]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"_id\": \"69\", \"timestamp\": \"59:51\", \"home\": {\"teamid\": 101, \"conference\": \"acc\", \"wins\": 847, \"losses\": 154, \"score\": 153, \"status\": \"winning\", \"players\": [{\"id\": 2, \"name\": \"sarah\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 2, \"name\": \"sarah\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 2, \"name\": \"sarah\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 2, \"name\": \"sarah\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 2, \"name\": \"sarah\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 2, \"name\": \"sarah\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 2, \"name\": \"sarah\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 2, \"name\": \"sarah\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 8, \"goals\": 6, \"pct\": 0.75}, {\"id\": 4, \"name\": \"stone\", \"shots\": 7, \"goals\": 1, \"pct\": 0.14285714285714285}, {\"id\": 4, \"name\": \"stone\", \"shots\": 7, \"goals\": 1, \"pct\": 0.14285714285714285}, {\"id\": 4, \"name\": \"stone\", \"shots\": 7, \"goals\": 1, \"pct\": 0.14285714285714285}, {\"id\": 4, \"name\": \"stone\", \"shots\": 7, \"goals\": 1, \"pct\": 0.14285714285714285}, {\"id\": 4, \"name\": \"stone\", \"shots\": 7, \"goals\": 1, \"pct\": 0.14285714285714285}, {\"id\": 4, \"name\": \"stone\", \"shots\": 7, \"goals\": 1, \"pct\": 0.14285714285714285}, {\"id\": 4, \"name\": \"stone\", \"shots\": 7, \"goals\": 1, \"pct\": 0.14285714285714285}, {\"id\": 1, \"name\": \"sam\", \"shots\": 4, \"goals\": 2, \"pct\": 0.5}, {\"id\": 1, \"name\": \"sam\", \"shots\": 4, \"goals\": 2, \"pct\": 0.5}, {\"id\": 1, \"name\": \"sam\", \"shots\": 4, \"goals\": 2, \"pct\": 0.5}, {\"id\": 1, \"name\": \"sam\", \"shots\": 4, \"goals\": 2, \"pct\": 0.5}, {\"id\": 9, \"name\": \"shelly\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 14, \"name\": \"jane\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 9, \"name\": \"shelly\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 14, \"name\": \"jane\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 9, \"name\": \"shelly\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 14, \"name\": \"jane\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 7, \"name\": \"sol\", \"shots\": 5, \"goals\": 0, \"pct\": 0.0}, {\"id\": 12, \"name\": \"julie\", \"shots\": 5, \"goals\": 0, \"pct\": 0.0}, {\"id\": 7, \"name\": \"sol\", \"shots\": 5, \"goals\": 0, \"pct\": 0.0}, {\"id\": 12, \"name\": \"julie\", \"shots\": 5, \"goals\": 0, \"pct\": 0.0}, {\"id\": 7, \"name\": \"sol\", \"shots\": 5, \"goals\": 0, \"pct\": 0.0}, {\"id\": 12, \"name\": \"julie\", \"shots\": 5, \"goals\": 0, \"pct\": 0.0}, {\"id\": 7, \"name\": \"sol\", \"shots\": 5, \"goals\": 0, \"pct\": 0.0}, {\"id\": 12, \"name\": \"julie\", \"shots\": 5, \"goals\": 0, \"pct\": 0.0}, {\"id\": 7, \"name\": \"sol\", \"shots\": 5, \"goals\": 0, \"pct\": 0.0}, {\"id\": 12, \"name\": \"julie\", \"shots\": 5, \"goals\": 0, \"pct\": 0.0}, {\"id\": 5, \"name\": \"sean\", \"shots\": 2, \"goals\": 0, \"pct\": 0.0}, {\"id\": 17, \"name\": \"james\", \"shots\": 2, \"goals\": 0, \"pct\": 0.0}, {\"id\": 5, \"name\": \"sean\", \"shots\": 2, \"goals\": 0, \"pct\": 0.0}, {\"id\": 17, \"name\": \"james\", \"shots\": 2, \"goals\": 0, \"pct\": 0.0}, {\"id\": 8, \"name\": \"shree\", \"shots\": 5, \"goals\": 1, \"pct\": 0.2}, {\"id\": 8, \"name\": \"shree\", \"shots\": 5, \"goals\": 1, \"pct\": 0.2}, {\"id\": 8, \"name\": \"shree\", \"shots\": 5, \"goals\": 1, \"pct\": 0.2}, {\"id\": 8, \"name\": \"shree\", \"shots\": 5, \"goals\": 1, \"pct\": 0.2}, {\"id\": 8, \"name\": \"shree\", \"shots\": 5, \"goals\": 1, \"pct\": 0.2}, {\"id\": 6, \"name\": \"sly\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 16, \"name\": \"julie\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 6, \"name\": \"sly\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 16, \"name\": \"julie\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 6, \"name\": \"sly\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 16, \"name\": \"julie\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 6, \"name\": \"sly\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 16, \"name\": \"julie\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 10, \"name\": \"swede\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 10, \"name\": \"swede\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 10, \"name\": \"swede\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 3, \"name\": \"steve\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 13, \"name\": \"james\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 3, \"name\": \"steve\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 13, \"name\": \"james\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 3, \"name\": \"steve\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 13, \"name\": \"james\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 3, \"name\": \"steve\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 13, \"name\": \"james\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 3, \"name\": \"steve\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 13, \"name\": \"james\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 3, \"name\": \"steve\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 13, \"name\": \"james\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 3, \"name\": \"steve\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}, {\"id\": 13, \"name\": \"james\", \"shots\": 7, \"goals\": 2, \"pct\": 0.2857142857142857}]}, \"away\": {\"teamid\": 205, \"conference\": \"big10\", \"wins\": 351, \"losses\": 156, \"score\": 44, \"status\": \"losing\", \"players\": [{\"id\": 20, \"name\": \"julie\", \"shots\": 1, \"goals\": 0, \"pct\": 0.0}, {\"id\": 2, \"name\": \"sarah\", \"shots\": 3, \"goals\": 3, \"pct\": 1.0}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 3, \"goals\": 3, \"pct\": 1.0}, {\"id\": 2, \"name\": \"sarah\", \"shots\": 3, \"goals\": 3, \"pct\": 1.0}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 3, \"goals\": 3, \"pct\": 1.0}, {\"id\": 2, \"name\": \"sarah\", \"shots\": 3, \"goals\": 3, \"pct\": 1.0}, {\"id\": 11, \"name\": \"jimmy\", \"shots\": 3, \"goals\": 3, \"pct\": 1.0}, {\"id\": 15, \"name\": \"jimmy\", \"shots\": 1, \"goals\": 0, \"pct\": 0.0}, {\"id\": 18, \"name\": \"jane\", \"shots\": 1, \"goals\": 0, \"pct\": 0.0}, {\"id\": 19, \"name\": \"jimmy\", \"shots\": 2, \"goals\": 1, \"pct\": 0.5}, {\"id\": 19, \"name\": \"jimmy\", \"shots\": 2, \"goals\": 1, \"pct\": 0.5}, {\"id\": 9, \"name\": \"shelly\", \"shots\": 2, \"goals\": 2, \"pct\": 1.0}, {\"id\": 14, \"name\": \"jane\", \"shots\": 2, \"goals\": 2, \"pct\": 1.0}, {\"id\": 9, \"name\": \"shelly\", \"shots\": 2, \"goals\": 2, \"pct\": 1.0}, {\"id\": 14, \"name\": \"jane\", \"shots\": 2, \"goals\": 2, \"pct\": 1.0}, {\"id\": 7, \"name\": \"sol\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 12, \"name\": \"julie\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 7, \"name\": \"sol\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 12, \"name\": \"julie\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 7, \"name\": \"sol\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 12, \"name\": \"julie\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 7, \"name\": \"sol\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 12, \"name\": \"julie\", \"shots\": 4, \"goals\": 0, \"pct\": 0.0}, {\"id\": 5, \"name\": \"sean\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 17, \"name\": \"james\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 5, \"name\": \"sean\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 17, \"name\": \"james\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 5, \"name\": \"sean\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 17, \"name\": \"james\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 6, \"name\": \"sly\", \"shots\": 2, \"goals\": 1, \"pct\": 0.5}, {\"id\": 16, \"name\": \"julie\", \"shots\": 2, \"goals\": 1, \"pct\": 0.5}, {\"id\": 6, \"name\": \"sly\", \"shots\": 2, \"goals\": 1, \"pct\": 0.5}, {\"id\": 16, \"name\": \"julie\", \"shots\": 2, \"goals\": 1, \"pct\": 0.5}, {\"id\": 3, \"name\": \"steve\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 13, \"name\": \"james\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 3, \"name\": \"steve\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 13, \"name\": \"james\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 3, \"name\": \"steve\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}, {\"id\": 13, \"name\": \"james\", \"shots\": 3, \"goals\": 1, \"pct\": 0.3333333333333333}]}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from pyspark.sql.functions import col, collect_list, struct, sum as spark_sum, max as spark_max, when\n",
        "import json\n",
        "\n",
        "\n",
        "home_team = boxScores.filter(col(\"teamId\") == 101).agg(\n",
        "    spark_max(\"latest_event_id\").alias(\"latest_event_id\"),\n",
        "    spark_max(\"latest_event_timestamp\").alias(\"latest_event_timestamp\"),\n",
        "    spark_max(\"conference\").alias(\"conference\"),\n",
        "    spark_sum(\"wins\").alias(\"total_wins\"),\n",
        "    spark_sum(\"losses\").alias(\"total_losses\"),\n",
        "    spark_sum(\"totalGoals\").alias(\"total_Goals\"),\n",
        "    collect_list(\n",
        "        struct(\n",
        "            col(\"playerId\").alias(\"id\"),\n",
        "            col(\"name\"),\n",
        "            col(\"totalShots\").alias(\"shots\"),\n",
        "            col(\"totalGoals\").alias(\"goals\"),\n",
        "            (col(\"totalGoals\") / col(\"totalShots\")).alias(\"pct\")\n",
        "        )\n",
        "    ).alias(\"players\")\n",
        ").collect()[0]\n",
        "\n",
        "away_team = boxScores.filter(col(\"teamId\") == 205).agg(\n",
        "    spark_max(\"latest_event_id\").alias(\"latest_event_id\"),\n",
        "    spark_max(\"latest_event_timestamp\").alias(\"latest_event_timestamp\"),\n",
        "    spark_max(\"conference\").alias(\"conference\"),\n",
        "    spark_sum(\"wins\").alias(\"total_wins\"),\n",
        "    spark_sum(\"losses\").alias(\"total_losses\"),\n",
        "    spark_sum(\"totalGoals\").alias(\"total_Goals\"),\n",
        "    collect_list(\n",
        "        struct(\n",
        "            col(\"playerId\").alias(\"id\"),\n",
        "            col(\"name\"),\n",
        "            col(\"totalShots\").alias(\"shots\"),\n",
        "            col(\"totalGoals\").alias(\"goals\"),\n",
        "            (col(\"totalGoals\") / col(\"totalShots\")).alias(\"pct\")\n",
        "        )\n",
        "    ).alias(\"players\")\n",
        ").collect()[0]\n",
        "\n",
        "home_doc = {\n",
        "    \"_id\": str(home_team[\"latest_event_id\"]),\n",
        "    \"timestamp\": home_team[\"latest_event_timestamp\"],\n",
        "    \"home\": {\n",
        "        \"teamid\": 101,\n",
        "        \"conference\": home_team[\"conference\"],\n",
        "        \"wins\": home_team[\"total_wins\"],\n",
        "        \"losses\": home_team[\"total_losses\"],\n",
        "        \"score\": home_team[\"total_Goals\"],\n",
        "        \"status\": \"winning\" if home_team[\"total_Goals\"] > away_team[\"total_Goals\"] else \"losing\" if home_team[\"total_Goals\"] < away_team[\"total_Goals\"] else \"tied\",\n",
        "        \"players\": [\n",
        "            {\n",
        "                \"id\": player[\"id\"],\n",
        "                \"name\": player[\"name\"],\n",
        "                \"shots\": player[\"shots\"],\n",
        "                \"goals\": player[\"goals\"],\n",
        "                \"pct\": player[\"pct\"]\n",
        "            }\n",
        "            for player in home_team[\"players\"]\n",
        "        ]\n",
        "    },\n",
        "    \"away\": {\n",
        "        \"teamid\": 205,\n",
        "        \"conference\": away_team[\"conference\"],\n",
        "        \"wins\": away_team[\"total_wins\"],\n",
        "        \"losses\": away_team[\"total_losses\"],\n",
        "        \"score\": away_team[\"total_Goals\"],\n",
        "        \"status\": \"losing\" if home_team[\"total_Goals\"] > away_team[\"total_Goals\"] else \"winning\" if home_team[\"total_Goals\"] < away_team[\"total_Goals\"] else \"tied\",\n",
        "        \"players\": [\n",
        "            {\n",
        "                \"id\": player[\"id\"],\n",
        "                \"name\": player[\"name\"],\n",
        "                \"shots\": player[\"shots\"],\n",
        "                \"goals\": player[\"goals\"],\n",
        "                \"pct\": player[\"pct\"]\n",
        "            }\n",
        "            for player in away_team[\"players\"]\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "json_data = json.dumps(home_doc)\n",
        "\n",
        "\n",
        "print(json_data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd637a91",
      "metadata": {
        "id": "fd637a91",
        "outputId": "8487283d-d786-4b14-f11e-8ba8bb3f3c60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/03/09 04:21:44 ERROR Executor: Exception in task 0.0 in stage 3745.0 (TID 108234)\n",
            "com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n",
            "\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n",
            "\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n",
            "\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n",
            "\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n",
            "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n",
            "\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n",
            "\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n",
            "\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n",
            "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n",
            "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
            "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
            "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
            "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n",
            "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n",
            "\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n",
            "\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n",
            "\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n",
            "\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n",
            "\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n",
            "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n",
            "\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "24/03/09 04:21:44 ERROR TaskSetManager: Task 0 in stage 3745.0 failed 1 times; aborting job\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o4008.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3745.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3745.0 (TID 108234) (jupyter executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_6568/3586622571.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Write the DataFrame to MongoDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mongo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mongodb://localhost:27017/sidearm.boxscores\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4008.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3745.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3745.0 (TID 108234) (jupyter executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=localhost:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.ConnectException: Connection refused (Connection refused)}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)\n\tat com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)\n\tat com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df = spark.createDataFrame([json_data], \"string\")\n",
        "\n",
        "\n",
        "df.write.format(\"mongo\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"uri\", \"mongodb://localhost:27017/sidearm.boxscores\") \\\n",
        "    .save()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd45f490-d3f1-4e36-8b75-a24e4e4149c1",
      "metadata": {
        "id": "fd45f490-d3f1-4e36-8b75-a24e4e4149c1",
        "outputId": "ed61b5f7-b714-424c-de32-42a6750b40df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _id: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            " |-- home_teamid: string (nullable = true)\n",
            " |-- home_conference: string (nullable = true)\n",
            " |-- home_wins: string (nullable = true)\n",
            " |-- home_losses: string (nullable = true)\n",
            " |-- home_score: string (nullable = true)\n",
            " |-- home_status: string (nullable = true)\n",
            " |-- home_players: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- goals: long (nullable = true)\n",
            " |    |    |-- id: long (nullable = true)\n",
            " |    |    |-- name: string (nullable = true)\n",
            " |    |    |-- pct: double (nullable = true)\n",
            " |    |    |-- shots: long (nullable = true)\n",
            " |-- away_teamid: string (nullable = true)\n",
            " |-- away_conference: string (nullable = true)\n",
            " |-- away_wins: string (nullable = true)\n",
            " |-- away_losses: string (nullable = true)\n",
            " |-- away_score: string (nullable = true)\n",
            " |-- away_status: string (nullable = true)\n",
            " |-- away_players: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- goals: long (nullable = true)\n",
            " |    |    |-- id: long (nullable = true)\n",
            " |    |    |-- name: string (nullable = true)\n",
            " |    |    |-- pct: double (nullable = true)\n",
            " |    |    |-- shots: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#  Alternate code\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = spark.read.json(spark.sparkContext.parallelize([json_data]))\n",
        "\n",
        "\n",
        "df_exploded = df.select(\n",
        "    col(\"_id\"),\n",
        "    col(\"timestamp\"),\n",
        "    col(\"home.teamid\").alias(\"home_teamid\"),\n",
        "    col(\"home.conference\").alias(\"home_conference\"),\n",
        "    col(\"home.wins\").alias(\"home_wins\"),\n",
        "    col(\"home.losses\").alias(\"home_losses\"),\n",
        "    col(\"home.score\").alias(\"home_score\"),\n",
        "    col(\"home.status\").alias(\"home_status\"),\n",
        "    col(\"home.players\").alias(\"home_players\"),\n",
        "    col(\"away.teamid\").alias(\"away_teamid\"),\n",
        "    col(\"away.conference\").alias(\"away_conference\"),\n",
        "    col(\"away.wins\").alias(\"away_wins\"),\n",
        "    col(\"away.losses\").alias(\"away_losses\"),\n",
        "    col(\"away.score\").alias(\"away_score\"),\n",
        "    col(\"away.status\").alias(\"away_status\"),\n",
        "    col(\"away.players\").alias(\"away_players\")\n",
        ")\n",
        "\n",
        "\n",
        "df_exploded = df_exploded.withColumn(\"home_teamid\", col(\"home_teamid\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"home_wins\", col(\"home_wins\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"home_losses\", col(\"home_losses\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"home_score\", col(\"home_score\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_teamid\", col(\"away_teamid\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_wins\", col(\"away_wins\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_losses\", col(\"away_losses\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_score\", col(\"away_score\").cast(\"string\"))\n",
        "\n",
        "\n",
        "df_exploded.printSchema()\n",
        "\n",
        "df_exploded.write.format(\"mongo\").mode(\"append\").option(\"database\",\"sidearm\").option(\"collection\",\"boxscores\").save()\n",
        "\n",
        "\n",
        "# Here i have seriliazed the data and selected the necessary columns and renamed the columns along with casting them into string and then uploading it in the mongodb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35cf344a",
      "metadata": {
        "id": "35cf344a"
      },
      "outputs": [],
      "source": [
        "\n",
        "dfNew= gamestream.selectExpr(\n",
        "  \"split(value, '[ ]')[0] as eventId\",\n",
        "  \"split(value, '[ ]')[1] as eventTimestamp\",\n",
        "  \"split(value, '[ ]')[2] as teamId\",\n",
        "  \"split(value, '[ ]')[3] as jerseyNumber\",\n",
        "  \"split(value, '[ ]')[4] as goal\")\n",
        "#dfNew.show()\n",
        "\n",
        "dfNew = dfNew.withColumn(\"eventId\", col(\"eventId\").cast(\"int\"))\\\n",
        "                     .withColumn(\"eventTimestamp\", col(\"eventTimestamp\").cast(\"string\"))\\\n",
        "                     .withColumn(\"teamId\", col(\"teamId\").cast(\"int\"))\\\n",
        "                     .withColumn(\"jerseyNumber\", col(\"jerseyNumber\").cast(\"int\"))\\\n",
        "                     .withColumn(\"goal\", col(\"goal\").cast(\"int\"))\n",
        "\n",
        "dfNew.show()\n",
        "\n",
        "# Here we are extracting values from gamestream.txt . The values are present inside the gamestream are sepearted with space so we are making it as seperate columns based on this\n",
        "# and we are renaming the columns based on our requirements and finally we are displaying the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea5d68ac",
      "metadata": {
        "id": "ea5d68ac"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.functions import col, sum as spark_sum,count\n",
        "\n",
        "playerDF = dfNew.groupBy(\"teamId\", \"jerseyNumber\") \\\n",
        "    .agg(spark_sum(\"goal\").alias(\"totalGoals\"), count(col(\"goal\").cast(\"int\")).alias(\"totalShots\"))\n",
        "\n",
        "teamDF = dfNew.groupBy(\"teamId\") \\\n",
        "    .agg(spark_sum(\"goal\").alias(\"totalTeamGoals\"))\n",
        "\n",
        "finalstatsdf = playerDF.join(teamDF, \"teamId\", \"inner\")\n",
        "\n",
        "finalstatsdf.show()\n",
        "\n",
        "# Here we are grouping by teamid and jerseyNumber of the players taken from the gamestream.txt file from s3 bucket gamestream in minio and then we are aggregrating the goal columns\n",
        "# based on the above grouping condition as total goals of a certain player and counting the goals as the total shots of the players. Then we are again grouping the dfNew\n",
        "# dataframe based on teamid to calculate the totalTeam goals of the home team and away team , Then we are joining these two tables based on teamid to get the resultant dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86db4f09",
      "metadata": {
        "id": "86db4f09"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Joining the two DataFrames\n",
        "joineddf = dfNew.join(finalstatsdf, [\"teamId\", \"jerseyNumber\"], \"inner\")\n",
        "\n",
        "# Group by teamId to find the most current event ID and timestamp\n",
        "latest_event_df = joineddf.groupBy(\"teamId\").agg(\n",
        "    F.max(\"eventId\").alias(\"latest_event_id\"),\n",
        "    F.max(\"eventTimestamp\").alias(\"latest_event_timestamp\")\n",
        ")\n",
        "\n",
        "# Drop duplicate values based on teamId\n",
        "latest_event_df = latest_event_df.dropDuplicates([\"teamId\"])\n",
        "\n",
        "# Joining the joineddf DataFrame with latest_event_df\n",
        "resultdf = joineddf.join(\n",
        "    latest_event_df,\n",
        "    joineddf.teamId == latest_event_df.teamId,\n",
        "    \"inner\"\n",
        ").select(\n",
        "    joineddf[\"teamId\"],\n",
        "    joineddf[\"jerseyNumber\"],\n",
        "    joineddf[\"eventId\"],\n",
        "    joineddf[\"eventTimestamp\"],\n",
        "    joineddf[\"totalGoals\"],\n",
        "    joineddf[\"totalShots\"],\n",
        "    joineddf[\"totalTeamGoals\"],\n",
        "    latest_event_df[\"latest_event_id\"],\n",
        "    latest_event_df[\"latest_event_timestamp\"]\n",
        ")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "\n",
        "resultdf.select(\n",
        "    joineddf[\"teamId\"],\n",
        "    joineddf[\"jerseyNumber\"],\n",
        "    joineddf[\"totalGoals\"],\n",
        "    joineddf[\"totalShots\"],\n",
        "    joineddf[\"totalTeamGoals\"],\n",
        "    latest_event_df[\"latest_event_id\"],\n",
        "    latest_event_df[\"latest_event_timestamp\"]\n",
        ").show()\n",
        "\n",
        "# Here we are joining the dfNew dataframe from question 3 and finalstatsdf dataframe from question 4  and then we are grouping the resultant dataframe by teamid and using aggregrate\n",
        "# function max to extract the most current timestamp and eventId. We are joining the joinedDf and the resultant dataframe to obtain all the values for the corresponding latest eventid\n",
        "# and timestamp . We are displaying the required colums by using select statement and aliasing the column name as per the question given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8cc24d8",
      "metadata": {
        "id": "e8cc24d8"
      },
      "outputs": [],
      "source": [
        "\n",
        "teamsdf = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"teams\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .load()\n",
        "\n",
        "boxPlayers = resultdf.join(df,\n",
        "                           (resultdf.jerseyNumber == df.number),  # Add comma here\n",
        "                           \"left\") \\\n",
        "                     .select(resultdf[\"*\"], df[\"id\"].alias(\"playerId\"), df[\"name\"])\n",
        "\n",
        "# Joining team statistics with team reference data\n",
        "boxScores = boxPlayers.join(teamsdf, boxPlayers.teamId == teamsdf.id, \"inner\") \\\n",
        "    .select(boxPlayers[\"*\"], teamsdf[\"name\"].alias(\"teamName\"), teamsdf[\"conference\"], teamsdf[\"wins\"], teamsdf[\"losses\"])\n",
        "\n",
        "\n",
        "boxScores.show()\n",
        "\n",
        "# Here we are joining the result df with player table df using playerid and jerseynumber and then displaying all the values and aliasing id value to playerid to avoid ambiguity.\n",
        "# Then we are joining the boxplayer table with teamsdf table using teamid  and then displaying all the values that are necessary for updating the boxscores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca6d2c4",
      "metadata": {
        "id": "bca6d2c4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from pyspark.sql.functions import col, collect_list, struct, sum as spark_sum, max as spark_max, when\n",
        "import json\n",
        "\n",
        "\n",
        "home_team = boxScores.filter(col(\"teamId\") == 101).agg(\n",
        "    spark_max(\"latest_event_id\").alias(\"latest_event_id\"),\n",
        "    spark_max(\"latest_event_timestamp\").alias(\"latest_event_timestamp\"),\n",
        "    spark_max(\"conference\").alias(\"conference\"),\n",
        "    spark_sum(\"wins\").alias(\"total_wins\"),\n",
        "    spark_sum(\"losses\").alias(\"total_losses\"),\n",
        "    spark_sum(\"totalGoals\").alias(\"total_Goals\"),\n",
        "    collect_list(\n",
        "        struct(\n",
        "            col(\"playerId\").alias(\"id\"),\n",
        "            col(\"name\"),\n",
        "            col(\"totalShots\").alias(\"shots\"),\n",
        "            col(\"totalGoals\").alias(\"goals\"),\n",
        "            (col(\"totalGoals\") / col(\"totalShots\")).alias(\"pct\")\n",
        "        )\n",
        "    ).alias(\"players\")\n",
        ").collect()[0]\n",
        "\n",
        "away_team = boxScores.filter(col(\"teamId\") == 205).agg(\n",
        "    spark_max(\"latest_event_id\").alias(\"latest_event_id\"),\n",
        "    spark_max(\"latest_event_timestamp\").alias(\"latest_event_timestamp\"),\n",
        "    spark_max(\"conference\").alias(\"conference\"),\n",
        "    spark_sum(\"wins\").alias(\"total_wins\"),\n",
        "    spark_sum(\"losses\").alias(\"total_losses\"),\n",
        "    spark_sum(\"totalGoals\").alias(\"total_Goals\"),\n",
        "    collect_list(\n",
        "        struct(\n",
        "            col(\"playerId\").alias(\"id\"),\n",
        "            col(\"name\"),\n",
        "            col(\"totalShots\").alias(\"shots\"),\n",
        "            col(\"totalGoals\").alias(\"goals\"),\n",
        "            (col(\"totalGoals\") / col(\"totalShots\")).alias(\"pct\")\n",
        "        )\n",
        "    ).alias(\"players\")\n",
        ").collect()[0]\n",
        "\n",
        "home_doc = {\n",
        "    \"_id\": str(home_team[\"latest_event_id\"]),\n",
        "    \"timestamp\": home_team[\"latest_event_timestamp\"],\n",
        "    \"home\": {\n",
        "        \"teamid\": 101,\n",
        "        \"conference\": home_team[\"conference\"],\n",
        "        \"wins\": home_team[\"total_wins\"],\n",
        "        \"losses\": home_team[\"total_losses\"],\n",
        "        \"score\": home_team[\"total_Goals\"],\n",
        "        \"status\": \"winning\" if home_team[\"total_Goals\"] > away_team[\"total_Goals\"] else \"losing\" if home_team[\"total_Goals\"] < away_team[\"total_Goals\"] else \"tied\",\n",
        "        \"players\": [\n",
        "            {\n",
        "                \"id\": player[\"id\"],\n",
        "                \"name\": player[\"name\"],\n",
        "                \"shots\": player[\"shots\"],\n",
        "                \"goals\": player[\"goals\"],\n",
        "                \"pct\": player[\"pct\"]\n",
        "            }\n",
        "            for player in home_team[\"players\"]\n",
        "        ]\n",
        "    },\n",
        "    \"away\": {\n",
        "        \"teamid\": 205,\n",
        "        \"conference\": away_team[\"conference\"],\n",
        "        \"wins\": away_team[\"total_wins\"],\n",
        "        \"losses\": away_team[\"total_losses\"],\n",
        "        \"score\": away_team[\"total_Goals\"],\n",
        "        \"status\": \"losing\" if home_team[\"total_Goals\"] > away_team[\"total_Goals\"] else \"winning\" if home_team[\"total_Goals\"] < away_team[\"total_Goals\"] else \"tied\",\n",
        "        \"players\": [\n",
        "            {\n",
        "                \"id\": player[\"id\"],\n",
        "                \"name\": player[\"name\"],\n",
        "                \"shots\": player[\"shots\"],\n",
        "                \"goals\": player[\"goals\"],\n",
        "                \"pct\": player[\"pct\"]\n",
        "            }\n",
        "            for player in away_team[\"players\"]\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "json_data = json.dumps(home_doc)\n",
        "\n",
        "\n",
        "print(json_data)\n",
        "\n",
        "# Here initially we are creating home team and away team stats by filtering the boxscores based on the team id and aggregrating the following ones based on the latest event id ,\n",
        "# timestamp and wins& losses and total goals which is aggregrated by sum function to find out total team goals for the home tean  .Playerspecific data is aggregated into a list,\n",
        "#including each player's ID, name, total shots attempted, total goals scored and shooting percentage (calculated as the ratio of total goals to total shots). Same is calculated for the\n",
        "# away team . Then we are creating a structure for the json document and specifying the condition for the status of the game , if the total goals of home team is greater than losing team\n",
        "# we consider the status as wining else consider as losing , if its neither of the following scenario it is considered as tie. We are using the json.dumps function where it converts\n",
        "# the corresponding document of list to json data and finally we are printing the json data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff5f9330",
      "metadata": {
        "id": "ff5f9330"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = spark.read.json(spark.sparkContext.parallelize([json_data]))\n",
        "\n",
        "\n",
        "df_exploded = df.select(\n",
        "    col(\"_id\"),\n",
        "    col(\"timestamp\"),\n",
        "    col(\"home.teamid\").alias(\"home_teamid\"),\n",
        "    col(\"home.conference\").alias(\"home_conference\"),\n",
        "    col(\"home.wins\").alias(\"home_wins\"),\n",
        "    col(\"home.losses\").alias(\"home_losses\"),\n",
        "    col(\"home.score\").alias(\"home_score\"),\n",
        "    col(\"home.status\").alias(\"home_status\"),\n",
        "    col(\"home.players\").alias(\"home_players\"),\n",
        "    col(\"away.teamid\").alias(\"away_teamid\"),\n",
        "    col(\"away.conference\").alias(\"away_conference\"),\n",
        "    col(\"away.wins\").alias(\"away_wins\"),\n",
        "    col(\"away.losses\").alias(\"away_losses\"),\n",
        "    col(\"away.score\").alias(\"away_score\"),\n",
        "    col(\"away.status\").alias(\"away_status\"),\n",
        "    col(\"away.players\").alias(\"away_players\")\n",
        ")\n",
        "\n",
        "\n",
        "df_exploded = df_exploded.withColumn(\"home_teamid\", col(\"home_teamid\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"home_wins\", col(\"home_wins\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"home_losses\", col(\"home_losses\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"home_score\", col(\"home_score\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_teamid\", col(\"away_teamid\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_wins\", col(\"away_wins\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_losses\", col(\"away_losses\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_score\", col(\"away_score\").cast(\"string\"))\n",
        "\n",
        "\n",
        "df_exploded.printSchema()\n",
        "\n",
        "df_exploded.write.format(\"mongo\").mode(\"append\").option(\"database\",\"sidearm\").option(\"collection\",\"boxscores\").save()\n",
        "\n",
        "\n",
        "# Here i have seriliazed the data and selected the necessary columns and renamed the columns along with casting them into string and then we are writing the following output to mongodb\n",
        "#which stores this specific data into database name in sidearm and in collection boxscores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10472e4e",
      "metadata": {
        "id": "10472e4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.functions import col, sum as spark_sum,count\n",
        "\n",
        "playerDF = dfNew.groupBy(\"teamId\", \"jerseyNumber\") \\\n",
        "    .agg(spark_sum(\"goal\").alias(\"totalGoals\"), count(col(\"goal\").cast(\"int\")).alias(\"totalShots\"))\n",
        "\n",
        "teamDF = dfNew.groupBy(\"teamId\") \\\n",
        "    .agg(spark_sum(\"goal\").alias(\"totalTeamGoals\"))\n",
        "\n",
        "finalstatsdf = playerDF.join(teamDF, \"teamId\", \"inner\")\n",
        "\n",
        "finalstatsdf.show()\n",
        "\n",
        "# Step 2 : Extracting the most recent timestamp and event id\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "joineddf = dfNew.join(finalstatsdf, [\"teamId\", \"jerseyNumber\"], \"inner\")\n",
        "\n",
        "\n",
        "latest_event_df = joineddf.groupBy(\"teamId\").agg(\n",
        "    F.max(\"eventId\").alias(\"latest_event_id\"),\n",
        "    F.max(\"eventTimestamp\").alias(\"latest_event_timestamp\")\n",
        ")\n",
        "\n",
        "\n",
        "latest_event_df = latest_event_df.dropDuplicates([\"teamId\"])\n",
        "\n",
        "\n",
        "resultdf = joineddf.join(\n",
        "    latest_event_df,\n",
        "    joineddf.teamId == latest_event_df.teamId,\n",
        "    \"inner\"\n",
        ").select(\n",
        "    joineddf[\"teamId\"],\n",
        "    joineddf[\"jerseyNumber\"],\n",
        "    joineddf[\"eventId\"],\n",
        "    joineddf[\"eventTimestamp\"],\n",
        "    joineddf[\"totalGoals\"],\n",
        "    joineddf[\"totalShots\"],\n",
        "    joineddf[\"totalTeamGoals\"],\n",
        "    latest_event_df[\"latest_event_id\"],\n",
        "    latest_event_df[\"latest_event_timestamp\"]\n",
        ")\n",
        "\n",
        "\n",
        "resultdf.select(\n",
        "    joineddf[\"teamId\"],\n",
        "    joineddf[\"jerseyNumber\"],\n",
        "    joineddf[\"totalGoals\"],\n",
        "    joineddf[\"totalShots\"],\n",
        "    joineddf[\"totalTeamGoals\"],\n",
        "    latest_event_df[\"latest_event_id\"],\n",
        "    latest_event_df[\"latest_event_timestamp\"]\n",
        ").show()\n",
        "teamsdf = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"teams\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .load()\n",
        "\n",
        "boxPlayers = resultdf.join(df,\n",
        "                           (resultdf.jerseyNumber == df.number),  # Add comma here\n",
        "                           \"left\") \\\n",
        "                     .select(resultdf[\"*\"], df[\"id\"].alias(\"playerId\"), df[\"name\"])\n",
        "\n",
        "\n",
        "boxScores = boxPlayers.join(teamsdf, boxPlayers.teamId == teamsdf.id, \"inner\") \\\n",
        "    .select(boxPlayers[\"*\"], teamsdf[\"name\"].alias(\"teamName\"), teamsdf[\"conference\"], teamsdf[\"wins\"], teamsdf[\"losses\"])\n",
        "\n",
        "\n",
        "boxScores.show()\n",
        "\n",
        "# Step 3 : Creating the Json document\n",
        "from pyspark.sql.functions import col, collect_list, struct, sum as spark_sum, max as spark_max, when\n",
        "import json\n",
        "\n",
        "\n",
        "home_team = boxScores.filter(col(\"teamId\") == 101).agg(\n",
        "    spark_max(\"latest_event_id\").alias(\"latest_event_id\"),\n",
        "    spark_max(\"latest_event_timestamp\").alias(\"latest_event_timestamp\"),\n",
        "    spark_max(\"conference\").alias(\"conference\"),\n",
        "    spark_sum(\"wins\").alias(\"total_wins\"),\n",
        "    spark_sum(\"losses\").alias(\"total_losses\"),\n",
        "    spark_sum(\"totalGoals\").alias(\"total_Goals\"),\n",
        "    collect_list(\n",
        "        struct(\n",
        "            col(\"playerId\").alias(\"id\"),\n",
        "            col(\"name\"),\n",
        "            col(\"totalShots\").alias(\"shots\"),\n",
        "            col(\"totalGoals\").alias(\"goals\"),\n",
        "            (col(\"totalGoals\") / col(\"totalShots\")).alias(\"pct\")\n",
        "        )\n",
        "    ).alias(\"players\")\n",
        ").collect()[0]\n",
        "\n",
        "away_team = boxScores.filter(col(\"teamId\") == 205).agg(\n",
        "    spark_max(\"latest_event_id\").alias(\"latest_event_id\"),\n",
        "    spark_max(\"latest_event_timestamp\").alias(\"latest_event_timestamp\"),\n",
        "    spark_max(\"conference\").alias(\"conference\"),\n",
        "    spark_sum(\"wins\").alias(\"total_wins\"),\n",
        "    spark_sum(\"losses\").alias(\"total_losses\"),\n",
        "    spark_sum(\"totalGoals\").alias(\"total_Goals\"),\n",
        "    collect_list(\n",
        "        struct(\n",
        "            col(\"playerId\").alias(\"id\"),\n",
        "            col(\"name\"),\n",
        "            col(\"totalShots\").alias(\"shots\"),\n",
        "            col(\"totalGoals\").alias(\"goals\"),\n",
        "            (col(\"totalGoals\") / col(\"totalShots\")).alias(\"pct\")\n",
        "        )\n",
        "    ).alias(\"players\")\n",
        ").collect()[0]\n",
        "\n",
        "home_doc = {\n",
        "    \"_id\": str(home_team[\"latest_event_id\"]),\n",
        "    \"timestamp\": home_team[\"latest_event_timestamp\"],\n",
        "    \"home\": {\n",
        "        \"teamid\": 101,\n",
        "        \"conference\": home_team[\"conference\"],\n",
        "        \"wins\": home_team[\"total_wins\"],\n",
        "        \"losses\": home_team[\"total_losses\"],\n",
        "        \"score\": home_team[\"total_Goals\"],\n",
        "        \"status\": \"winning\" if home_team[\"total_Goals\"] > away_team[\"total_Goals\"] else \"losing\" if home_team[\"total_Goals\"] < away_team[\"total_Goals\"] else \"tied\",\n",
        "        \"players\": [\n",
        "            {\n",
        "                \"id\": player[\"id\"],\n",
        "                \"name\": player[\"name\"],\n",
        "                \"shots\": player[\"shots\"],\n",
        "                \"goals\": player[\"goals\"],\n",
        "                \"pct\": player[\"pct\"]\n",
        "            }\n",
        "            for player in home_team[\"players\"]\n",
        "        ]\n",
        "    },\n",
        "    \"away\": {\n",
        "        \"teamid\": 205,\n",
        "        \"conference\": away_team[\"conference\"],\n",
        "        \"wins\": away_team[\"total_wins\"],\n",
        "        \"losses\": away_team[\"total_losses\"],\n",
        "        \"score\": away_team[\"total_Goals\"],\n",
        "        \"status\": \"losing\" if home_team[\"total_Goals\"] > away_team[\"total_Goals\"] else \"winning\" if home_team[\"total_Goals\"] < away_team[\"total_Goals\"] else \"tied\",\n",
        "        \"players\": [\n",
        "            {\n",
        "                \"id\": player[\"id\"],\n",
        "                \"name\": player[\"name\"],\n",
        "                \"shots\": player[\"shots\"],\n",
        "                \"goals\": player[\"goals\"],\n",
        "                \"pct\": player[\"pct\"]\n",
        "            }\n",
        "            for player in away_team[\"players\"]\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "json_data = json.dumps(home_doc)\n",
        "\n",
        "\n",
        "print(json_data)\n",
        "\n",
        "# Step 4 : Writing the Json string to mongodb\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = spark.read.json(spark.sparkContext.parallelize([json_data]))\n",
        "\n",
        "\n",
        "df_exploded = df.select(\n",
        "    col(\"_id\"),\n",
        "    col(\"timestamp\"),\n",
        "    col(\"home.teamid\").alias(\"home_teamid\"),\n",
        "    col(\"home.conference\").alias(\"home_conference\"),\n",
        "    col(\"home.wins\").alias(\"home_wins\"),\n",
        "    col(\"home.losses\").alias(\"home_losses\"),\n",
        "    col(\"home.score\").alias(\"home_score\"),\n",
        "    col(\"home.status\").alias(\"home_status\"),\n",
        "    col(\"home.players\").alias(\"home_players\"),\n",
        "    col(\"away.teamid\").alias(\"away_teamid\"),\n",
        "    col(\"away.conference\").alias(\"away_conference\"),\n",
        "    col(\"away.wins\").alias(\"away_wins\"),\n",
        "    col(\"away.losses\").alias(\"away_losses\"),\n",
        "    col(\"away.score\").alias(\"away_score\"),\n",
        "    col(\"away.status\").alias(\"away_status\"),\n",
        "    col(\"away.players\").alias(\"away_players\")\n",
        ")\n",
        "\n",
        "\n",
        "df_exploded = df_exploded.withColumn(\"home_teamid\", col(\"home_teamid\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"home_wins\", col(\"home_wins\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"home_losses\", col(\"home_losses\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"home_score\", col(\"home_score\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_teamid\", col(\"away_teamid\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_wins\", col(\"away_wins\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_losses\", col(\"away_losses\").cast(\"string\"))\n",
        "df_exploded = df_exploded.withColumn(\"away_score\", col(\"away_score\").cast(\"string\"))\n",
        "\n",
        "\n",
        "df_exploded.printSchema()\n",
        "\n",
        "df_exploded.write.format(\"mongo\").mode(\"append\").option(\"database\",\"sidearm\").option(\"collection\",\"boxscores\").save()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# This is combined python script for extracting the text from minio and converting it to dataframe  and performing necessary operations to obtain the most current timestamp of the\n",
        "# ganestream and then creating a resultant dataframe boxscores necessary for creating a json document boxscores and then we are updating that into mongodb into sidearm database\n",
        "# and in collection boxscores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039708a6",
      "metadata": {
        "id": "039708a6",
        "outputId": "c1549128-066b-4859-ac14-502032c0ce93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+----+------+\n",
            "|teamId|conference|wins|losses|\n",
            "+------+----------+----+------+\n",
            "|   205|     big10| 0.0|   0.0|\n",
            "|   101|       acc| 1.0|   0.0|\n",
            "+------+----------+----+------+\n",
            "\n",
            "+------+-------------+----------+---------+-----------+\n",
            "|teamId|         name|conference|team_wins|team_losses|\n",
            "+------+-------------+----------+---------+-----------+\n",
            "|   101|     syracuse|       acc|       11|          2|\n",
            "|   205|johns hopkins|     big10|        9|          4|\n",
            "+------+-------------+----------+---------+-----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+-------------+----+------+\n",
            "|teamId|conference|         name|wins|losses|\n",
            "+------+----------+-------------+----+------+\n",
            "|   205|     big10|johns hopkins| 9.0|   4.0|\n",
            "|   101|       acc|     syracuse|12.0|   2.0|\n",
            "+------+----------+-------------+----+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 2285:=====================================================>(74 + 1) / 75]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+-------------+----+------+\n",
            "| id|conference|         name|wins|losses|\n",
            "+---+----------+-------------+----+------+\n",
            "|205|     big10|johns hopkins| 9.0|   5.0|\n",
            "|101|       acc|     syracuse|12.0|   2.0|\n",
            "+---+----------+-------------+----+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from pyspark.sql.functions import col, when , max , min\n",
        "\n",
        "\n",
        "teamNewdf = boxScores.dropDuplicates([\"playerId\"])\n",
        "\n",
        "home_team_df = teamNewdf.filter(col(\"teamId\") == 101)\n",
        "away_team_df = teamNewdf.filter(col(\"teamId\") == 205)\n",
        "\n",
        "\n",
        "teamNewdf = teamNewdf.withColumn(\"wins\", when(home_team_df[\"totalTeamGoals\"] > away_team_df[\"totalTeamGoals\"], 1.0)\n",
        "                               .otherwise(0.0)) \\\n",
        "               .withColumn(\"losses\", when(home_team_df[\"totalTeamGoals\"] < away_team_df[\"totalTeamGoals\"], 1.0)\n",
        "                                  .otherwise(0.0))\n",
        "\n",
        "\n",
        "teamNewdf.show()\n",
        "\n",
        "\n",
        "teamNewdf = teamNewdf.withColumn(\"wins\",\n",
        "                                  when(teamNewdf[\"teamId\"] == 101,\n",
        "                                       when(teamNewdf[\"totalTeamGoals\"] > teamNewdf[\"totalGoals\"], 1.0).otherwise(0.0))\n",
        "                                  .otherwise(when(teamNewdf[\"teamId\"] == 205,\n",
        "                                                  when(teamNewdf[\"totalTeamGoals\"] < teamNewdf[\"totalGoals\"], 1.0).otherwise(0.0))\n",
        "                                             .otherwise(0.0))) \\\n",
        "                     .withColumn(\"losses\",\n",
        "                                 when(teamNewdf[\"teamId\"] == 101,\n",
        "                                      when(teamNewdf[\"totalTeamGoals\"] < teamNewdf[\"totalGoals\"], 1.0).otherwise(0.0))\n",
        "                                 .otherwise(when(teamNewdf[\"teamId\"] == 205,\n",
        "                                                 when(teamNewdf[\"totalTeamGoals\"] > teamNewdf[\"totalGoals\"], 1.0).otherwise(0.0))\n",
        "                                            .otherwise(0.0)))\n",
        "\n",
        "teamNewdf_selected = teamNewdf.select(\"teamId\", \"conference\", \"wins\", \"losses\")\n",
        "\n",
        "\n",
        "agg_teamNewdf = teamNewdf.groupBy(\"teamId\", \"conference\").agg({\"wins\": \"max\", \"losses\": \"min\"})\n",
        "agg_teamNewdf = agg_teamNewdf.withColumnRenamed(\"max(wins)\", \"wins\").withColumnRenamed(\"min(losses)\", \"losses\")\n",
        "\n",
        "agg_teamNewdf = agg_teamNewdf.select(\"teamId\", \"conference\", \"wins\", \"losses\")\n",
        "\n",
        "\n",
        "agg_teamNewdf.show()\n",
        "\n",
        "teamsdf.show()\n",
        "\n",
        "teamsdf = teamsdf.withColumnRenamed('id', 'teamId')\n",
        "\n",
        "\n",
        "updated_teamsdf = teamsdf.join(\n",
        "    agg_teamNewdf,\n",
        "    ['teamId', 'conference'],\n",
        "    'left_outer'\n",
        ").withColumn(\n",
        "    \"wins\",\n",
        "    teamsdf[\"team_wins\"] + col(\"wins\")\n",
        ").withColumn(\n",
        "    \"losses\",\n",
        "    teamsdf[\"team_losses\"] + col(\"losses\")\n",
        ").drop(\"team_wins\", \"team_losses\")\n",
        "\n",
        "\n",
        "updated_teamsdf.show()\n",
        "updated_teamsdf = updated_teamsdf.withColumnRenamed('teamId', 'id')\n",
        "\n",
        "# Here we are drop the duplicates values based on the playerid to eliminate the duplicate values and then we are calculating the wins and losses based on the condition we\n",
        "# used for the status field for the boxscores json document . Then we are extracting the necessary columns from the resultant dataframe and aggregrating the wins and losses\n",
        "# based on the teamid and conference. And then we are finally adding the wins and losses stats of the resultant dataframes to teams table to update the wins and loss of the\n",
        "# corresponding home and away teams. and we are displaying it . I have renamed some columns to avoid ambiguity error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef80bd86-e3e7-4239-bb13-0c488c91296a",
      "metadata": {
        "id": "ef80bd86-e3e7-4239-bb13-0c488c91296a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "updated_teamsdf.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"teams2\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .save()\n",
        "updated_teamsdf.show()\n",
        "\n",
        "\n",
        "# Here we are updating the newly updated stats of both home and away teams to mssql in a new table called players 2 . Here we are providing necessary configurations such as\n",
        "# url , password and user which grants the permission to write and load the values in mssql table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b963d91",
      "metadata": {
        "id": "9b963d91",
        "outputId": "9af3bd85-fe97-4de9-f713-b73530f3dc49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+-----+-----+------+\n",
            "| id|  name|number|shots|goals|teamid|\n",
            "+---+------+------+-----+-----+------+\n",
            "|  1|   sam|     6|   56|   23|   101|\n",
            "|  2| sarah|     1|   85|   34|   101|\n",
            "|  3| steve|     2|   60|   20|   101|\n",
            "|  4| stone|    13|   33|   10|   101|\n",
            "|  5|  sean|    17|   26|    9|   101|\n",
            "|  6|   sly|     8|   78|   15|   101|\n",
            "|  7|   sol|     9|   52|   20|   101|\n",
            "|  8| shree|     4|   20|    4|   101|\n",
            "|  9|shelly|    15|   10|    2|   101|\n",
            "| 10| swede|    10|   90|   50|   101|\n",
            "| 11| jimmy|     1|  100|   50|   205|\n",
            "| 12| julie|     9|   10|    0|   205|\n",
            "| 13| james|     2|   45|   15|   205|\n",
            "| 14|  jane|    15|   82|   46|   205|\n",
            "| 15| jimmy|    16|   42|   30|   205|\n",
            "| 16| julie|     8|   67|   32|   205|\n",
            "| 17| james|    17|   40|   14|   205|\n",
            "| 18|  jane|     3|   91|   40|   205|\n",
            "| 19| jimmy|     5|   78|   22|   205|\n",
            "| 20| julie|    22|   83|   19|   205|\n",
            "+---+------+------+-----+-----+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------+-------+--------------+----------+----------+--------------+---------------+----------------------+--------+-----+--------+----------+----+------+\n",
            "|teamGID|jerseyNumber|eventId|eventTimestamp|totalGoals|totalShots|totalTeamGoals|latest_event_id|latest_event_timestamp|playerId| name|teamName|conference|wins|losses|\n",
            "+-------+------------+-------+--------------+----------+----------+--------------+---------------+----------------------+--------+-----+--------+----------+----+------+\n",
            "|    101|           1|     67|         03:22|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     67|         03:22|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     54|         18:24|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     54|         18:24|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     52|         20:22|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     52|         20:22|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     24|         40:20|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     24|         40:20|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     20|         41:09|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     20|         41:09|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     16|         45:49|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     16|         45:49|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     10|         49:55|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|    101|           1|     10|         49:55|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|    101|           1|      4|         55:03|         6|         8|            14|             69|                 59:51|       2|sarah|syracuse|       acc|  11|     2|\n",
            "|    101|           1|      4|         55:03|         6|         8|            14|             69|                 59:51|      11|jimmy|syracuse|       acc|  11|     2|\n",
            "|    101|          13|     68|         00:42|         1|         7|            14|             69|                 59:51|       4|stone|syracuse|       acc|  11|     2|\n",
            "|    101|          13|     63|         07:42|         1|         7|            14|             69|                 59:51|       4|stone|syracuse|       acc|  11|     2|\n",
            "|    101|          13|     46|         25:17|         1|         7|            14|             69|                 59:51|       4|stone|syracuse|       acc|  11|     2|\n",
            "|    101|          13|     39|         30:26|         1|         7|            14|             69|                 59:51|       4|stone|syracuse|       acc|  11|     2|\n",
            "+-------+------------+-------+--------------+----------+----------+--------------+---------------+----------------------+--------+-----+--------+----------+----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 3540:===================================================>  (72 + 1) / 75]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+-----+-----+------+-------+------------+----------+----------+\n",
            "| id|  name|number|shots|goals|teamid|teamGId|jerseyNumber|totalGoals|totalShots|\n",
            "+---+------+------+-----+-----+------+-------+------------+----------+----------+\n",
            "| 12| julie|     9|   10|    0|   205|    205|           9|         0|        32|\n",
            "| 13| james|     2|   45|   15|   205|    205|           2|         6|        18|\n",
            "|  7|   sol|     9|   52|   20|   101|    101|           9|         0|        50|\n",
            "| 10| swede|    10|   90|   50|   101|    101|          10|         3|         9|\n",
            "|  3| steve|     2|   60|   20|   101|    101|           2|        28|        98|\n",
            "|  6|   sly|     8|   78|   15|   101|    101|           8|         0|        32|\n",
            "| 15| jimmy|    16|   42|   30|   205|    205|          16|         0|         1|\n",
            "|  4| stone|    13|   33|   10|   101|    101|          13|         7|        49|\n",
            "|  1|   sam|     6|   56|   23|   101|    101|           6|         8|        16|\n",
            "|  9|shelly|    15|   10|    2|   101|    101|          15|         6|        18|\n",
            "| 18|  jane|     3|   91|   40|   205|    205|           3|         0|         1|\n",
            "| 14|  jane|    15|   82|   46|   205|    205|          15|         8|         8|\n",
            "| 19| jimmy|     5|   78|   22|   205|    205|           5|         2|         4|\n",
            "|  2| sarah|     1|   85|   34|   101|    101|           1|        96|       128|\n",
            "| 11| jimmy|     1|  100|   50|   205|    205|           1|        18|        18|\n",
            "| 20| julie|    22|   83|   19|   205|    205|          22|         0|         1|\n",
            "| 17| james|    17|   40|   14|   205|    205|          17|         6|        18|\n",
            "| 16| julie|     8|   67|   32|   205|    205|           8|         4|         8|\n",
            "|  8| shree|     4|   20|    4|   101|    101|           4|         5|        25|\n",
            "|  5|  sean|    17|   26|    9|   101|    101|          17|         0|         8|\n",
            "+---+------+------+-----+-----+------+-------+------------+----------+----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from pyspark.sql.functions import sum, col, regexp_extract\n",
        "\n",
        "playerdf = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"players\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .load()\n",
        "\n",
        "playerdf.show()\n",
        "\n",
        "\n",
        "boxScores = boxScores.withColumnRenamed(\"teamId\", \"teamGID\")\n",
        "\n",
        "# Display the updated DataFrame\n",
        "boxScores.show()\n",
        "boxscores_df = boxScores.withColumn(\"totalGoals\", regexp_extract(col(\"totalGoals\"), \"\\\\d+\", 0).cast(\"int\")) \\\n",
        "                           .withColumn(\"totalShots\", regexp_extract(col(\"totalShots\"), \"\\\\d+\", 0).cast(\"int\"))\n",
        "\n",
        "# Group the boxscores DataFrame by teamId and jerseyNumber and calculate sum of totalGoals and totalShots\n",
        "boxscores_grouped_df = boxscores_df.groupBy(\"teamGId\", \"jerseyNumber\") \\\n",
        "    .agg(sum(\"totalGoals\").alias(\"totalGoals\"), sum(\"totalShots\").alias(\"totalShots\"))\n",
        "\n",
        "# Join the grouped boxscores DataFrame with the players DataFrame\n",
        "# Join the players DataFrame with the grouped boxscores DataFrame\n",
        "updated_players_df = playerdf.join(\n",
        "    boxscores_grouped_df,\n",
        "    (playerdf[\"teamid\"] == boxscores_grouped_df[\"teamGID\"]) & (playerdf[\"number\"] == boxscores_grouped_df[\"jerseyNumber\"]),\n",
        "    \"left_outer\"\n",
        ")\n",
        "\n",
        "\n",
        "# Display the updated DataFrame\n",
        "updated_players_df.show()\n",
        "\n",
        "# Here we are reading the players data once again to avoid confusion as we might have same name for different dataframes and i am renaming the teamid to teamGID to avoid ambiguity\n",
        "# Firstly This part casts the result of the regular expression extraction to an integer and renames it as totalGoals and regex part takes out the first occurence. Then we are aggregrating\n",
        "# it with teamGId and jerseyNumber with sum of goals and sum of shots. Then we are joining this data with players table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a048e59f",
      "metadata": {
        "id": "a048e59f",
        "outputId": "24892575-ac3d-4bcd-c1ef-83f57de6237f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+------+-------+------------+----------+----------+\n",
            "| id|  name|number|teamid|teamGId|jerseyNumber|totalGoals|totalShots|\n",
            "+---+------+------+------+-------+------------+----------+----------+\n",
            "| 12| julie|     9|   205|    205|           9|         0|        42|\n",
            "| 13| james|     2|   205|    205|           2|        21|        63|\n",
            "|  7|   sol|     9|   101|    101|           9|        20|       102|\n",
            "| 10| swede|    10|   101|    101|          10|        53|        99|\n",
            "|  3| steve|     2|   101|    101|           2|        48|       158|\n",
            "|  6|   sly|     8|   101|    101|           8|        15|       110|\n",
            "| 15| jimmy|    16|   205|    205|          16|        30|        43|\n",
            "|  4| stone|    13|   101|    101|          13|        17|        82|\n",
            "|  1|   sam|     6|   101|    101|           6|        31|        72|\n",
            "|  9|shelly|    15|   101|    101|          15|         8|        28|\n",
            "| 18|  jane|     3|   205|    205|           3|        40|        92|\n",
            "| 14|  jane|    15|   205|    205|          15|        54|        90|\n",
            "| 19| jimmy|     5|   205|    205|           5|        24|        82|\n",
            "|  2| sarah|     1|   101|    101|           1|       130|       213|\n",
            "| 11| jimmy|     1|   205|    205|           1|        68|       118|\n",
            "| 20| julie|    22|   205|    205|          22|        19|        84|\n",
            "| 17| james|    17|   205|    205|          17|        20|        58|\n",
            "| 16| julie|     8|   205|    205|           8|        36|        75|\n",
            "|  8| shree|     4|   101|    101|           4|         9|        45|\n",
            "|  5|  sean|    17|   101|    101|          17|         9|        34|\n",
            "+---+------+------+------+-------+------------+----------+----------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+----------+----------+------+\n",
            "| id|  name|number|totalShots|totalGoals|teamId|\n",
            "+---+------+------+----------+----------+------+\n",
            "| 12| julie|     9|        42|         0|   205|\n",
            "| 13| james|     2|        63|        21|   205|\n",
            "|  7|   sol|     9|       102|        20|   101|\n",
            "| 10| swede|    10|        99|        53|   101|\n",
            "|  3| steve|     2|       158|        48|   101|\n",
            "|  6|   sly|     8|       110|        15|   101|\n",
            "| 15| jimmy|    16|        43|        30|   205|\n",
            "|  4| stone|    13|        82|        17|   101|\n",
            "|  1|   sam|     6|        72|        31|   101|\n",
            "|  9|shelly|    15|        28|         8|   101|\n",
            "| 18|  jane|     3|        92|        40|   205|\n",
            "| 14|  jane|    15|        90|        54|   205|\n",
            "| 19| jimmy|     5|        82|        24|   205|\n",
            "|  2| sarah|     1|       213|       130|   101|\n",
            "| 11| jimmy|     1|       118|        68|   205|\n",
            "| 20| julie|    22|        84|        19|   205|\n",
            "| 17| james|    17|        58|        20|   205|\n",
            "| 16| julie|     8|        75|        36|   205|\n",
            "|  8| shree|     4|        45|         9|   101|\n",
            "|  5|  sean|    17|        34|         9|   101|\n",
            "+---+------+------+----------+----------+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+------+------+-----+-----+------+\n",
            "| id|  name|number|shots|goals|teamid|\n",
            "+---+------+------+-----+-----+------+\n",
            "| 12| julie|     9|   42|    0|   205|\n",
            "| 13| james|     2|   63|   21|   205|\n",
            "|  7|   sol|     9|  102|   20|   101|\n",
            "| 10| swede|    10|   99|   53|   101|\n",
            "|  3| steve|     2|  158|   48|   101|\n",
            "|  6|   sly|     8|  110|   15|   101|\n",
            "| 15| jimmy|    16|   43|   30|   205|\n",
            "|  4| stone|    13|   82|   17|   101|\n",
            "|  1|   sam|     6|   72|   31|   101|\n",
            "|  9|shelly|    15|   28|    8|   101|\n",
            "| 18|  jane|     3|   92|   40|   205|\n",
            "| 14|  jane|    15|   90|   54|   205|\n",
            "| 19| jimmy|     5|   82|   24|   205|\n",
            "|  2| sarah|     1|  213|  130|   101|\n",
            "| 11| jimmy|     1|  118|   68|   205|\n",
            "| 20| julie|    22|   84|   19|   205|\n",
            "| 17| james|    17|   58|   20|   205|\n",
            "| 16| julie|     8|   75|   36|   205|\n",
            "|  8| shree|     4|   45|    9|   101|\n",
            "|  5|  sean|    17|   34|    9|   101|\n",
            "+---+------+------+-----+-----+------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "# Calculate the new values for goals and shots\n",
        "updated_players_df = updated_players_df.withColumn(\n",
        "    \"totalGoals\",\n",
        "    F.coalesce(updated_players_df[\"goals\"] + F.col(\"totalGoals\"), updated_players_df[\"goals\"])\n",
        ").withColumn(\n",
        "    \"totalShots\",\n",
        "    F.coalesce(updated_players_df[\"shots\"] + F.col(\"totalShots\"), updated_players_df[\"shots\"])\n",
        ").drop(\"goals\", \"shots\")\n",
        "\n",
        "# Display the updated DataFrame\n",
        "updated_players_df.show()\n",
        "\n",
        "\n",
        "\n",
        "# Selecting the desired columns\n",
        "final_output_df = updated_players_df.select(\"id\", \"name\", \"number\", \"totalShots\", \"totalGoals\", \"teamId\")\n",
        "\n",
        "# Displaying the final output DataFrame\n",
        "final_output_df.show()\n",
        "\n",
        "final_output_df = final_output_df.select(\n",
        "    col(\"id\").alias(\"id\"),\n",
        "    col(\"name\").alias(\"name\"),\n",
        "    col(\"number\").alias(\"number\"),\n",
        "    col(\"totalShots\").alias(\"shots\"),\n",
        "    col(\"totalGoals\").alias(\"goals\"),\n",
        "    col(\"teamId\").alias(\"teamid\")\n",
        ")\n",
        "\n",
        "# Display the updated DataFrame\n",
        "final_output_df.show()\n",
        "\n",
        "\n",
        "final_output_df.write.format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"url\", mssql_url) \\\n",
        "    .option(\"dbtable\", \"players2\") \\\n",
        "    .option(\"user\", mssql_user) \\\n",
        "    .option(\"password\", mssql_pw) \\\n",
        "    .save()\n",
        "\n",
        "# Then we are adding the goals and shots with totalgoals and totalshots and we are finally droping those columns once the above procedure is done.\n",
        "#Then we are retreiving necessary columns from  updated players df and alising the columns as per the players table. We are writing the\n",
        "# resultant dataframe to players 2 with necessary configurations of mssql that grants permission to write in the table players 2."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}